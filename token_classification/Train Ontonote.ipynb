{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "traditional-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "persistent-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "local-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-fundamental",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "specialized-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(PATH,convert2bio=False):\n",
    "    sents = []\n",
    "    chunks = open(PATH,'r').read().split('\\n\\n')\n",
    "    for chunk in chunks:\n",
    "        lines = chunk.split('\\n')\n",
    "        sent = []\n",
    "        current_tag = None\n",
    "        previous_tag = None\n",
    "        for line in lines:\n",
    "            if line != '':\n",
    "                token = line.split('\\t')\n",
    "                previous_tag = current_tag \n",
    "                current_tag = token[1]\n",
    "                if convert2bio:\n",
    "                    if previous_tag == current_tag and current_tag != 'O':\n",
    "                        sent.append((token[0],'I-'+token[1]))\n",
    "                    elif previous_tag != current_tag and current_tag != 'O':\n",
    "                        sent.append((token[0],'B-'+token[1]))\n",
    "                    else:\n",
    "                        sent.append((token[0],token[1]))\n",
    "                else:\n",
    "                    sent.append((token[0],token[1]))\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "international-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_schema(samples):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    for sample in samples:\n",
    "        sent = []\n",
    "        tag = []\n",
    "        for s in sample:\n",
    "            sent.append(s[0])\n",
    "            tag.append(tag_index[s[1]])\n",
    "\n",
    "        sentences.append(sent)\n",
    "        tags.append(tag)\n",
    "\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "agreed-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = prepare_dataset('data/ontonotes5/ner_train.txt', convert2bio=True)\n",
    "valid_set = prepare_dataset('data/ontonotes5/ner_valid.txt', convert2bio=True)\n",
    "test_set = prepare_dataset('data/ontonotes5/ner_test.txt', convert2bio=True)\n",
    "\n",
    "samples = train_set + test_set + valid_set\n",
    "schema = ['_'] + sorted({tag for sentence in samples for _, tag in sentence})\n",
    "tag_index = {tag: i for i, tag in enumerate(schema)}\n",
    "index_tag = {i: tag for i, tag in enumerate(schema)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "known-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_tag = convert_schema(train_set)\n",
    "valid_sentences, valid_tag = convert_schema(valid_set)\n",
    "test_sentences, test_tag = convert_schema(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-lodging",
   "metadata": {},
   "source": [
    "# Prepare torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "restricted-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "educational-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntonoteDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len=128):\n",
    "        super(OntonoteDataset, self).__init__()\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        tokenized_input = self.tokenizer(\n",
    "            sentence, is_split_into_words=True, return_offsets_mapping=True,\n",
    "            padding='max_length', truncation=True, return_tensors='pt',\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        labels = []\n",
    "        previous_word_idx = None\n",
    "        label_all_tokens = True\n",
    "        \n",
    "        word_ids = tokenized_input.word_ids()\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(label[word_idx])\n",
    "            else:\n",
    "                labels.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        return {\n",
    "            'input_ids': tokenized_input['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokenized_input['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(labels)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adverse-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pytorch lightning\n",
    "class OntonoteDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, train_sentence, valid_sentence, test_sentence,\n",
    "        train_tag, valid_tag, test_tag,\n",
    "        tokenizer, batch_size=32, max_length=128\n",
    "    ):\n",
    "        super(OntonoteDataModule, self).__init__()\n",
    "        \n",
    "        self.train_sentence = train_sentence\n",
    "        self.train_tag = train_tag\n",
    "        self.valid_sentence = valid_sentence\n",
    "        self.valid_tag = valid_tag\n",
    "        self.test_sentence = test_sentence\n",
    "        self.test_tag = test_tag\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = OntonoteDataset(\n",
    "            self.train_sentence, self.train_tag, self.tokenizer\n",
    "        )\n",
    "        self.valid_dataset = OntonoteDataset(\n",
    "           self.valid_sentence, self.valid_tag, self.tokenizer\n",
    "        )\n",
    "        self.test_dataset = OntonoteDataset(\n",
    "           self.test_sentence, self.test_tag, self.tokenizer\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=batch_size, \n",
    "            sampler=train_sampler\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        valid_sampler = SequentialSampler(self.valid_dataset)\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=valid_sampler\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_sampler = SequentialSampler(self.test_dataset)\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=test_sampler\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "modified-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = OntonoteDataModule(\n",
    "    train_sentences, valid_sentences, test_sentences,\n",
    "    train_tag, valid_tag, test_tag,\n",
    "    tokenizer, batch_size\n",
    ")\n",
    "\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-withdrawal",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "private-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerClassifier(pl.LightningModule):\n",
    "    def __init__(self, n_classes=38):\n",
    "        super(NerClassifier, self).__init__()\n",
    "        self.bert_model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids, attention_mask)\n",
    "        return self.classifier(outputs[0])\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        logits = self(input_ids, attention_mask)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, self.n_classes)\n",
    "            active_labels = torch.where(\n",
    "                active_loss, labels.view(-1), \n",
    "                torch.tensor(self.criterion.ignore_index).type_as(labels)\n",
    "            )\n",
    "            loss = self.criterion(active_logits, active_labels)\n",
    "        else:\n",
    "            loss = self.criterion(logits.view(-1, self.n_classes), labels.view(-1))\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        logits = self(input_ids, attention_mask)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, self.n_classes)\n",
    "            active_labels = torch.where(\n",
    "                active_loss, labels.view(-1), \n",
    "                torch.tensor(self.criterion.ignore_index).type_as(labels)\n",
    "            )\n",
    "            loss = self.criterion(active_logits, active_labels)\n",
    "        else:\n",
    "            loss = self.criterion(logits.view(-1, self.n_classes), labels.view(-1))\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0, num_training_steps=epochs * len(data_module.train_dataloader())\n",
    "        )\n",
    "        \n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "proved-playing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = NerClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "coastal-party",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    gpus=1, progress_bar_refresh_rate=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bigger-japanese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | bert_model | DistilBertModel  | 134 M \n",
      "1 | classifier | Sequential       | 206 K \n",
      "2 | criterion  | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "134 M     Trainable params\n",
      "0         Non-trainable params\n",
      "134 M     Total params\n",
      "539.763   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8bdc26af004af4bf6ff349835b9e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/superceed1/anaconda3/envs/py38torch17/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/superceed1/anaconda3/envs/py38torch17/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046aabf07a914c6d9761ef3de02098c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/superceed1/anaconda3/envs/py38torch17/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "recent-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights/ontotnote_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
